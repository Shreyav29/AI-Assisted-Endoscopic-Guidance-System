# ðŸ§  AI-Assisted Endoscopic Guidance System

This project was developed in collaboration with a clinical institution to build an **AI-powered endoscopic guidance system** that can automatically **detect, label, and orient anatomical structures** in real time during endoscopic procedures.  
The goal was to assist clinicians with **real-time visual feedback**â€”labeling key throat structures and identifying the **cameraâ€™s directional orientation** (â€œTrue Northâ€) to improve procedural accuracy and efficiency.

---

## ðŸš€ Overview

The system combines two complementary AI components:

1. **Endoscopic Object Detection Model** â€“ a YOLOv8-based detection model to identify and label anatomical structures.
2. **True North Classification Model** â€“ a CNN-based classifier that determines the directional orientation of the endoscope.

These models together provide **spatial awareness** and **semantic understanding** of the endoscopic view, enabling clinicians to navigate efficiently during live procedures.

---

## ðŸ§© Problem Statement

Manual labeling of anatomical structures during live endoscopy is difficult, inconsistent, and heavily dependent on clinician experience.  
We aimed to design a real-time model capable of:
- Detecting and naming relevant throat and airway landmarks (epiglottis, glottis, arytenoids, vocal cords, trachea, etc.)
- Estimating the **directional alignment** of the endoscope (north/south/east/west/center)
- Operating efficiently on **mobile hardware** for real-time inference without cloud dependency

---

## ðŸ“Š Data Preparation

- **Dataset:** Thousands of endoscopic images collected under clinical supervision  
- **Annotation Format:** Expert-labeled XML annotations converted to YOLO format  
- **Preprocessing Pipeline:**
  - Parsed XML annotations  
  - Resized and letterboxed images to **416Ã—416**  
  - Merged left/right anatomical labels for better accuracy  
  - Validated annotation integrity  




#AI-Assisted Endoscopic Guidance System

##Project Overview
Our team partnered with a clinical institution to build a computer-vision system capable of
automatically identifying and labeling anatomical structures during endoscopic
procedures. The systemâ€™s goal was to assist clinicians by displaying real-time, labeled
bounding boxes around key throat and airway structures while also determining the
directional orientation of the endoscope.
Two complementary AI components were developed:
1. Endoscopic Object Detection Model â€“ a YOLOv8-based detection
system(classifier) for identifying anatomical structures.
2. True North Classification Model â€“ a lightweight classifier that determines the
relative viewing orientation (â€œTrue Northâ€) of the camera feed.

##Problem Statement
Manual labeling of internal structures during live endoscopic procedures is difficult,
inconsistent, and depends heavily on clinician experience.
We aimed to design a model capable of:
â€¢ Detecting and naming relevant anatomical landmarks such as the epiglottis, glottis,
arytenoids, vocal cords, and trachea in real-time.
â€¢ Understanding the directional alignment of the endoscope (north/south/east/west
center) to guide scope positioning.

##Data Preparation and Annotation
â€¢ Thousands of endoscopic images were collected internally under clinical
supervision.
â€¢ Images were annotated using XML format through expert manual labeling.
â€¢ Labels included both left/right variants of structures (e.g., Left Arytenoid / Right
Arytenoid), which were merged during preprocessing for better accuracy.
â€¢ A custom data preprocessing pipeline was built to handle:

o Parsing XML annotations
o Letterboxing and resizing to 416Ã—416
o Conversion to YOLO format (class_id x_center y_center width height)
o Validation of label integrity and consistency

##Model Development
Endoscopic YOLO Detection Model
â€¢ Architecture: YOLOv8s (small variant, CSPDarknet-inspired, anchor-free)
â€¢ Framework: PyTorch (Ultralytics YOLOv8 library)
â€¢ Input: 416Ã—416 RGB frames from endoscopic video
â€¢ Training: Fine-tuned on proprietary dataset for 100 epochs (Used GPU)
â€¢ Evaluation Metrics: Precision, Recall, F1-Score, and mean Average Precision
(mAP@0.5)
True North Classification Model
â€¢ Architecture: CNN classifier trained to predict one of 9 spatial zones (north, south,
east, west, and center combinations)
â€¢ Input: Single frame divided into 9 regions; class represents the quadrant containing
the known north point.
â€¢ Accuracy: ~92% on out-of-sample holdout set

##Results and Performance
Endoscopic YOLO Detection Model
â€¢ Achieved mean Average Precision (mAP@0.5) = 0.78 on holdout data.
â€¢ Demonstrated >90% precision across most anatomical structures.
â€¢ Primary confusions occur between left and right vocal cords or arytenoids, which
are visually symmetric.

â€¢ The modelâ€™s precision-recall trade-off curve indicated that at 80% recall, the
system maintained ~85% precision, suggesting balanced sensitivity and reliability
for clinical use.
True North Classifier
â€¢ Overall Accuracy: 92%
â€¢ High recall for dominant orientations (north-center, south-center)
â€¢ Lower performance on rare corner orientations (e.g., north-east, south-west) due to
class imbalance. We plan on retraining with larger dataset in the next phase of
training.

Deployment and Optimization
After training, the YOLOv8 model was exported and optimized for on-device deployment:
1. Exported to ONNX format using the Ultralytics API
2. Validated with ONNX Runtime for correctness
3. Converted to TorchScript and optimized with PyTorchâ€™s optimize_for_mobile utility
4. Packaged for mobile devices to enable real-time inference directly on endoscopic
camera systems without cloud latency
The resulting system runs locally on mobile hardware, labeling structures at near real-time
frame rates with minimal compute overhead.

Conclusion
The final integrated system successfully
â€¢ Identifies key throat structures,
â€¢ Labels them in real-time, and
â€¢ Assists clinicians with camera orientation feedback.
The combination of YOLOv8 object detection and the True North classifier provides both

spatial awareness and semantic understanding, demonstrating a foundation for AI-
assisted endoscopy.
